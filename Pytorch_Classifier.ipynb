{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier using PyTorch\n",
    "\n",
    "In this notebook we train an MLP classifier on the MINST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## replace with your own root directory\n",
    "ROOT=\"/Users/adnan/Programming for Data-Science/Artificial-Intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset. This will download a copy to your machine on first use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_set = torchvision.datasets.MNIST(\n",
    "    root='./classifier_data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(\n",
    "    root='./classifier_data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510,\n",
       "           0.0980, 0.3922, 0.4784, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.5922, 0.8157,\n",
       "           0.9882, 0.9882, 0.9882, 0.5725, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.1569, 0.5961, 0.9569, 0.9882, 0.9922,\n",
       "           0.8784, 0.8275, 0.9882, 0.9098, 0.1569, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0588, 0.5961, 0.9373, 0.9882, 0.9882, 0.9882, 0.8471,\n",
       "           0.1216, 0.1451, 0.9882, 0.9882, 0.2353, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3765, 0.9882, 0.9882, 0.9882, 0.9882, 0.8510, 0.1137,\n",
       "           0.0000, 0.1451, 0.9882, 0.9882, 0.2353, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.7098, 0.9882, 0.9882, 0.8627, 0.6549, 0.1176, 0.0000,\n",
       "           0.0000, 0.3020, 0.9882, 0.9882, 0.2353, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.1020, 0.5020, 0.2275, 0.0863, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.3922, 0.9882, 0.9882, 0.2353, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.6157, 0.9882, 0.9882, 0.2353, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4314, 0.4745, 0.4784,\n",
       "           0.4745, 0.7922, 0.9882, 0.7608, 0.0118, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0392, 0.2078, 0.7020, 0.9922, 0.9922, 1.0000,\n",
       "           0.9922, 0.9922, 0.8941, 0.1373, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0196, 0.2118, 0.8902, 0.9882, 0.9529, 0.8941, 0.6667, 0.9490,\n",
       "           0.9882, 0.9882, 0.9059, 0.4588, 0.0235, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n",
       "           0.3059, 0.9882, 0.9882, 0.4902, 0.2314, 0.0000, 0.0706, 0.8157,\n",
       "           0.9882, 0.9882, 0.9882, 0.9882, 0.3412, 0.0275, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.5294,\n",
       "           0.9882, 0.9882, 0.7059, 0.0627, 0.0000, 0.0824, 0.7961, 0.9922,\n",
       "           0.9686, 0.5059, 0.6784, 0.9882, 0.9882, 0.7216, 0.2588, 0.1922,\n",
       "           0.1922, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.5333, 0.9882,\n",
       "           0.9451, 0.4157, 0.0667, 0.0000, 0.2078, 0.7843, 0.9882, 0.8471,\n",
       "           0.2549, 0.0000, 0.0549, 0.2824, 0.6392, 0.9451, 0.9882, 0.9882,\n",
       "           0.8745, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4118, 0.9882, 0.9490,\n",
       "           0.3451, 0.0706, 0.2863, 0.6667, 0.9569, 0.9882, 0.4941, 0.1137,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3490, 0.7059, 0.7059,\n",
       "           0.1451, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9059, 0.9882, 0.9608,\n",
       "           0.8039, 0.8471, 0.9882, 0.9882, 0.9882, 0.4863, 0.0118, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8118, 0.9882, 0.9882,\n",
       "           0.9882, 0.9882, 0.6980, 0.4549, 0.1412, 0.0157, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510, 0.3647, 0.5608,\n",
       "           0.4745, 0.0902, 0.0235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the subset value and check how a tensor array is classified to a number from 0-9\n",
    "train_set[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "data, label = train_set[4]\n",
    "print(data.size()) # (Color,Width,Height)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some iterable Data Loaders for easy iteration on mini-batches during training and testing. Also, initialise an array with the 10 class IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size=24, # Forward pass only so batch size can be larger\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "classes = np.arange(0, 10)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images and labels as a sanity check.\n",
    "Use `torchvision.utils.make_grid` to create one image from a set of images. Note that this function converts single channel (grey-scale) tensors to have three channels. This is done by replicating the values into red, green and blue channels.\n",
    "\n",
    "Grey -> RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1 7 4 7 1 9 1 0 2 0 4 1 1 7 2\n"
     ]
    }
   ],
   "source": [
    "def timshow(x):\n",
    "    xa = np.transpose(x.numpy(),(1,2,0)) # Transpose bcz it wants -> (Width,Height,Color)\n",
    "    plt.imshow(xa) \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return xa\n",
    "    \n",
    "# get a batch of random training examples (images and corresponding labels)\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images and labels\n",
    "print(images.size())\n",
    "timshow(torchvision.utils.make_grid(images))\n",
    "print(*labels.numpy())     # asterisk unpacks the ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a classifier\n",
    "We use the `torch.nn` and `torch.optim` packages. These provide a simple way to build networks without losing sight of the iterative steps in gradient descent.\n",
    "\n",
    "First we construct the classifer function using the nn.Sequential wrapper that simply sequences the steps in the classifier function. In the case of a linear classifier there is just one nn.Linear layer. This is preceeded by `nn.Flatten` that vectorises a $28\\times28$ input image into a 1D vector of length $28*28$. We will also experiment with a two layer classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 1-layer\n",
    "net_1layer = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    \n",
    "#     single layer\n",
    "    nn.Linear(28*28, 10)\n",
    "    \n",
    "    # two layers\n",
    "#     nn.Linear(28*28, 300),\n",
    "#     nn.Sigmoid(),\n",
    "#     nn.Linear(300,10)\n",
    ")\n",
    "\n",
    "for param in net_1layer.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 784])\n",
      "torch.Size([300])\n",
      "torch.Size([10, 300])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 2-layers\n",
    "net_2layer = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    \n",
    "#     single layer\n",
    "#    nn.Linear(28*28, 10)\n",
    "    \n",
    "#    two layers\n",
    "    nn.Linear(28*28, 300),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(300,10)\n",
    ")\n",
    "\n",
    "for param in net_2layer.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network. For the two-layer network you'll need at least 200 epochs. 50 epochs will be more than enough for the one-layer network, but we'll run for the same number of epochs as for the two-layer network to give the full curve in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss:  0.614\n",
      "epoch: 2 loss:  0.389\n",
      "epoch: 3 loss:  0.354\n",
      "epoch: 4 loss:  0.336\n",
      "epoch: 5 loss:  0.324\n",
      "epoch: 6 loss:  0.316\n",
      "epoch: 7 loss:  0.309\n",
      "epoch: 8 loss:  0.304\n",
      "epoch: 9 loss:  0.300\n",
      "epoch: 10 loss:  0.296\n",
      "epoch: 11 loss:  0.293\n",
      "epoch: 12 loss:  0.291\n",
      "epoch: 13 loss:  0.288\n",
      "epoch: 14 loss:  0.286\n",
      "epoch: 15 loss:  0.284\n",
      "epoch: 16 loss:  0.283\n",
      "epoch: 17 loss:  0.281\n",
      "epoch: 18 loss:  0.279\n",
      "epoch: 19 loss:  0.278\n",
      "epoch: 20 loss:  0.277\n",
      "epoch: 21 loss:  0.276\n",
      "epoch: 22 loss:  0.275\n",
      "epoch: 23 loss:  0.274\n",
      "epoch: 24 loss:  0.273\n",
      "epoch: 25 loss:  0.272\n",
      "epoch: 26 loss:  0.271\n",
      "epoch: 27 loss:  0.270\n",
      "epoch: 28 loss:  0.270\n",
      "epoch: 29 loss:  0.269\n",
      "epoch: 30 loss:  0.268\n",
      "epoch: 31 loss:  0.267\n",
      "epoch: 32 loss:  0.267\n",
      "epoch: 33 loss:  0.266\n",
      "epoch: 34 loss:  0.266\n",
      "epoch: 35 loss:  0.265\n",
      "epoch: 36 loss:  0.265\n",
      "epoch: 37 loss:  0.264\n",
      "epoch: 38 loss:  0.264\n",
      "epoch: 39 loss:  0.263\n",
      "epoch: 40 loss:  0.263\n",
      "epoch: 41 loss:  0.262\n",
      "epoch: 42 loss:  0.262\n",
      "epoch: 43 loss:  0.261\n",
      "epoch: 44 loss:  0.261\n",
      "epoch: 45 loss:  0.261\n",
      "epoch: 46 loss:  0.260\n",
      "epoch: 47 loss:  0.260\n",
      "epoch: 48 loss:  0.259\n",
      "epoch: 49 loss:  0.259\n",
      "epoch: 50 loss:  0.259\n",
      "epoch: 51 loss:  0.258\n",
      "epoch: 52 loss:  0.258\n",
      "epoch: 53 loss:  0.257\n",
      "epoch: 54 loss:  0.257\n",
      "epoch: 55 loss:  0.257\n",
      "epoch: 56 loss:  0.257\n",
      "epoch: 57 loss:  0.256\n",
      "epoch: 58 loss:  0.256\n",
      "epoch: 59 loss:  0.256\n",
      "epoch: 60 loss:  0.256\n",
      "epoch: 61 loss:  0.255\n",
      "epoch: 62 loss:  0.255\n",
      "epoch: 63 loss:  0.255\n",
      "epoch: 64 loss:  0.254\n",
      "epoch: 65 loss:  0.254\n",
      "epoch: 66 loss:  0.254\n",
      "epoch: 67 loss:  0.254\n",
      "epoch: 68 loss:  0.254\n",
      "epoch: 69 loss:  0.253\n",
      "epoch: 70 loss:  0.253\n",
      "epoch: 71 loss:  0.253\n",
      "epoch: 72 loss:  0.253\n",
      "epoch: 73 loss:  0.253\n",
      "epoch: 74 loss:  0.252\n",
      "epoch: 75 loss:  0.252\n",
      "epoch: 76 loss:  0.252\n",
      "epoch: 77 loss:  0.252\n",
      "epoch: 78 loss:  0.252\n",
      "epoch: 79 loss:  0.251\n",
      "epoch: 80 loss:  0.251\n",
      "epoch: 81 loss:  0.251\n",
      "epoch: 82 loss:  0.251\n",
      "epoch: 83 loss:  0.251\n",
      "epoch: 84 loss:  0.250\n",
      "epoch: 85 loss:  0.250\n",
      "epoch: 86 loss:  0.250\n",
      "epoch: 87 loss:  0.250\n",
      "epoch: 88 loss:  0.250\n",
      "epoch: 89 loss:  0.250\n",
      "epoch: 90 loss:  0.249\n",
      "epoch: 91 loss:  0.249\n",
      "epoch: 92 loss:  0.249\n",
      "epoch: 93 loss:  0.249\n",
      "epoch: 94 loss:  0.249\n",
      "epoch: 95 loss:  0.249\n",
      "epoch: 96 loss:  0.248\n",
      "epoch: 97 loss:  0.248\n",
      "epoch: 98 loss:  0.248\n",
      "epoch: 99 loss:  0.248\n",
      "epoch: 100 loss:  0.248\n",
      "epoch: 101 loss:  0.248\n",
      "epoch: 102 loss:  0.248\n",
      "epoch: 103 loss:  0.247\n",
      "epoch: 104 loss:  0.247\n",
      "epoch: 105 loss:  0.247\n",
      "epoch: 106 loss:  0.247\n",
      "epoch: 107 loss:  0.247\n",
      "epoch: 108 loss:  0.247\n",
      "epoch: 109 loss:  0.247\n",
      "epoch: 110 loss:  0.246\n",
      "epoch: 111 loss:  0.246\n",
      "epoch: 112 loss:  0.246\n",
      "epoch: 113 loss:  0.246\n",
      "epoch: 114 loss:  0.246\n",
      "epoch: 115 loss:  0.246\n",
      "epoch: 116 loss:  0.246\n",
      "epoch: 117 loss:  0.246\n",
      "epoch: 118 loss:  0.246\n",
      "epoch: 119 loss:  0.245\n",
      "epoch: 120 loss:  0.245\n",
      "epoch: 121 loss:  0.245\n",
      "epoch: 122 loss:  0.245\n",
      "epoch: 123 loss:  0.245\n",
      "epoch: 124 loss:  0.245\n",
      "epoch: 125 loss:  0.245\n",
      "epoch: 126 loss:  0.245\n",
      "epoch: 127 loss:  0.245\n",
      "epoch: 128 loss:  0.244\n",
      "epoch: 129 loss:  0.244\n",
      "epoch: 130 loss:  0.244\n",
      "epoch: 131 loss:  0.244\n",
      "epoch: 132 loss:  0.244\n",
      "epoch: 133 loss:  0.244\n",
      "epoch: 134 loss:  0.244\n",
      "epoch: 135 loss:  0.244\n",
      "epoch: 136 loss:  0.244\n",
      "epoch: 137 loss:  0.244\n",
      "epoch: 138 loss:  0.244\n",
      "epoch: 139 loss:  0.243\n",
      "epoch: 140 loss:  0.243\n",
      "epoch: 141 loss:  0.243\n",
      "epoch: 142 loss:  0.243\n",
      "epoch: 143 loss:  0.243\n",
      "epoch: 144 loss:  0.243\n",
      "epoch: 145 loss:  0.243\n",
      "epoch: 146 loss:  0.243\n",
      "epoch: 147 loss:  0.243\n",
      "epoch: 148 loss:  0.243\n",
      "epoch: 149 loss:  0.243\n",
      "epoch: 150 loss:  0.242\n",
      "epoch: 151 loss:  0.242\n",
      "epoch: 152 loss:  0.242\n",
      "epoch: 153 loss:  0.242\n",
      "epoch: 154 loss:  0.242\n",
      "epoch: 155 loss:  0.242\n",
      "epoch: 156 loss:  0.242\n",
      "epoch: 157 loss:  0.242\n",
      "epoch: 158 loss:  0.242\n",
      "epoch: 159 loss:  0.242\n",
      "epoch: 160 loss:  0.241\n",
      "epoch: 161 loss:  0.241\n",
      "epoch: 162 loss:  0.241\n",
      "epoch: 163 loss:  0.241\n",
      "epoch: 164 loss:  0.241\n",
      "epoch: 165 loss:  0.241\n",
      "epoch: 166 loss:  0.241\n",
      "epoch: 167 loss:  0.241\n",
      "epoch: 168 loss:  0.241\n",
      "epoch: 169 loss:  0.241\n",
      "epoch: 170 loss:  0.241\n",
      "epoch: 171 loss:  0.241\n",
      "epoch: 172 loss:  0.241\n",
      "epoch: 173 loss:  0.240\n",
      "epoch: 174 loss:  0.241\n",
      "epoch: 175 loss:  0.240\n",
      "epoch: 176 loss:  0.240\n",
      "epoch: 177 loss:  0.240\n",
      "epoch: 178 loss:  0.240\n",
      "epoch: 179 loss:  0.240\n",
      "epoch: 180 loss:  0.240\n",
      "epoch: 181 loss:  0.240\n",
      "epoch: 182 loss:  0.240\n",
      "epoch: 183 loss:  0.240\n",
      "epoch: 184 loss:  0.240\n",
      "epoch: 185 loss:  0.240\n",
      "epoch: 186 loss:  0.240\n",
      "epoch: 187 loss:  0.240\n",
      "epoch: 188 loss:  0.239\n",
      "epoch: 189 loss:  0.240\n",
      "epoch: 190 loss:  0.239\n",
      "epoch: 191 loss:  0.239\n",
      "epoch: 192 loss:  0.239\n",
      "epoch: 193 loss:  0.239\n",
      "epoch: 194 loss:  0.239\n",
      "epoch: 195 loss:  0.239\n",
      "epoch: 196 loss:  0.239\n",
      "epoch: 197 loss:  0.239\n",
      "epoch: 198 loss:  0.239\n",
      "epoch: 199 loss:  0.239\n",
      "epoch: 200 loss:  0.239\n"
     ]
    }
   ],
   "source": [
    "nepochs = 200    # number of epochs\n",
    "results_path = 'linear1layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Note, this takes log(Softmax) and Log-Likelihood\n",
    "                                # Meaning, it takes out the prob. of an input to be equal to a certain output\n",
    "                                # and tell us of the loss..how much it is not so\n",
    "optimizer = optim.SGD(net_1layer.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net_1layer(inputs) # A map of input(784) - output(10) is made..\n",
    "                              # connections have what is called weights/coeffs, imagine like a computational graph\n",
    "                              # each of 784 points connected to 10 points, we need gradient of each now,\n",
    "                              # To Optimise.\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # Assigns gradients to each connection/weight/coeff. of the input - output in a Matrix\n",
    "        optimizer.step() # Updates the matrix with new weights, considering the step size..like p = p - stepsize*grad\n",
    "\n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net_1layer.state_dict(), \"losses\": losses}, results_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first dimension of inputs and outputs corresponds to a minibatch of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size: torch.Size([16, 1, 28, 28]), output size: torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "print(f\"input size: {inputs.size()}, output size: {outputs.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss:  1.372\n",
      "epoch: 2 loss:  0.540\n",
      "epoch: 3 loss:  0.415\n",
      "epoch: 4 loss:  0.369\n",
      "epoch: 5 loss:  0.344\n",
      "epoch: 6 loss:  0.328\n",
      "epoch: 7 loss:  0.316\n",
      "epoch: 8 loss:  0.307\n",
      "epoch: 9 loss:  0.299\n",
      "epoch: 10 loss:  0.293\n",
      "epoch: 11 loss:  0.287\n",
      "epoch: 12 loss:  0.282\n",
      "epoch: 13 loss:  0.277\n",
      "epoch: 14 loss:  0.272\n",
      "epoch: 15 loss:  0.267\n",
      "epoch: 16 loss:  0.262\n",
      "epoch: 17 loss:  0.258\n",
      "epoch: 18 loss:  0.253\n",
      "epoch: 19 loss:  0.248\n",
      "epoch: 20 loss:  0.244\n",
      "epoch: 21 loss:  0.240\n",
      "epoch: 22 loss:  0.235\n",
      "epoch: 23 loss:  0.231\n",
      "epoch: 24 loss:  0.226\n",
      "epoch: 25 loss:  0.222\n",
      "epoch: 26 loss:  0.217\n",
      "epoch: 27 loss:  0.213\n",
      "epoch: 28 loss:  0.209\n",
      "epoch: 29 loss:  0.206\n",
      "epoch: 30 loss:  0.202\n",
      "epoch: 31 loss:  0.198\n",
      "epoch: 32 loss:  0.194\n",
      "epoch: 33 loss:  0.191\n",
      "epoch: 34 loss:  0.187\n",
      "epoch: 35 loss:  0.184\n",
      "epoch: 36 loss:  0.180\n",
      "epoch: 37 loss:  0.177\n",
      "epoch: 38 loss:  0.174\n",
      "epoch: 39 loss:  0.171\n",
      "epoch: 40 loss:  0.168\n",
      "epoch: 41 loss:  0.165\n",
      "epoch: 42 loss:  0.163\n",
      "epoch: 43 loss:  0.160\n",
      "epoch: 44 loss:  0.157\n",
      "epoch: 45 loss:  0.155\n",
      "epoch: 46 loss:  0.152\n",
      "epoch: 47 loss:  0.150\n",
      "epoch: 48 loss:  0.148\n",
      "epoch: 49 loss:  0.145\n",
      "epoch: 50 loss:  0.143\n",
      "epoch: 51 loss:  0.141\n",
      "epoch: 52 loss:  0.139\n",
      "epoch: 53 loss:  0.137\n",
      "epoch: 54 loss:  0.135\n",
      "epoch: 55 loss:  0.133\n",
      "epoch: 56 loss:  0.131\n",
      "epoch: 57 loss:  0.129\n",
      "epoch: 58 loss:  0.127\n",
      "epoch: 59 loss:  0.125\n",
      "epoch: 60 loss:  0.124\n",
      "epoch: 61 loss:  0.122\n",
      "epoch: 62 loss:  0.120\n",
      "epoch: 63 loss:  0.118\n",
      "epoch: 64 loss:  0.117\n",
      "epoch: 65 loss:  0.115\n",
      "epoch: 66 loss:  0.114\n",
      "epoch: 67 loss:  0.112\n",
      "epoch: 68 loss:  0.111\n",
      "epoch: 69 loss:  0.110\n",
      "epoch: 70 loss:  0.108\n",
      "epoch: 71 loss:  0.107\n",
      "epoch: 72 loss:  0.106\n",
      "epoch: 73 loss:  0.104\n",
      "epoch: 74 loss:  0.103\n",
      "epoch: 75 loss:  0.102\n",
      "epoch: 76 loss:  0.100\n",
      "epoch: 77 loss:  0.099\n",
      "epoch: 78 loss:  0.098\n",
      "epoch: 79 loss:  0.097\n",
      "epoch: 80 loss:  0.096\n",
      "epoch: 81 loss:  0.095\n",
      "epoch: 82 loss:  0.094\n",
      "epoch: 83 loss:  0.092\n",
      "epoch: 84 loss:  0.091\n",
      "epoch: 85 loss:  0.090\n",
      "epoch: 86 loss:  0.089\n",
      "epoch: 87 loss:  0.088\n",
      "epoch: 88 loss:  0.087\n",
      "epoch: 89 loss:  0.086\n",
      "epoch: 90 loss:  0.085\n",
      "epoch: 91 loss:  0.084\n",
      "epoch: 92 loss:  0.084\n",
      "epoch: 93 loss:  0.083\n",
      "epoch: 94 loss:  0.082\n",
      "epoch: 95 loss:  0.081\n",
      "epoch: 96 loss:  0.080\n",
      "epoch: 97 loss:  0.079\n",
      "epoch: 98 loss:  0.078\n",
      "epoch: 99 loss:  0.078\n",
      "epoch: 100 loss:  0.077\n",
      "epoch: 101 loss:  0.076\n",
      "epoch: 102 loss:  0.075\n",
      "epoch: 103 loss:  0.075\n",
      "epoch: 104 loss:  0.074\n",
      "epoch: 105 loss:  0.073\n",
      "epoch: 106 loss:  0.072\n",
      "epoch: 107 loss:  0.072\n",
      "epoch: 108 loss:  0.071\n",
      "epoch: 109 loss:  0.070\n",
      "epoch: 110 loss:  0.070\n",
      "epoch: 111 loss:  0.069\n",
      "epoch: 112 loss:  0.068\n",
      "epoch: 113 loss:  0.067\n",
      "epoch: 114 loss:  0.067\n",
      "epoch: 115 loss:  0.066\n",
      "epoch: 116 loss:  0.066\n",
      "epoch: 117 loss:  0.065\n",
      "epoch: 118 loss:  0.064\n",
      "epoch: 119 loss:  0.064\n",
      "epoch: 120 loss:  0.063\n",
      "epoch: 121 loss:  0.063\n",
      "epoch: 122 loss:  0.062\n",
      "epoch: 123 loss:  0.061\n",
      "epoch: 124 loss:  0.061\n",
      "epoch: 125 loss:  0.060\n",
      "epoch: 126 loss:  0.060\n",
      "epoch: 127 loss:  0.059\n",
      "epoch: 128 loss:  0.059\n",
      "epoch: 129 loss:  0.058\n",
      "epoch: 130 loss:  0.058\n",
      "epoch: 131 loss:  0.057\n",
      "epoch: 132 loss:  0.057\n",
      "epoch: 133 loss:  0.056\n",
      "epoch: 134 loss:  0.056\n",
      "epoch: 135 loss:  0.055\n",
      "epoch: 136 loss:  0.055\n",
      "epoch: 137 loss:  0.054\n",
      "epoch: 138 loss:  0.054\n",
      "epoch: 139 loss:  0.053\n",
      "epoch: 140 loss:  0.053\n",
      "epoch: 141 loss:  0.052\n",
      "epoch: 142 loss:  0.052\n",
      "epoch: 143 loss:  0.052\n",
      "epoch: 144 loss:  0.051\n",
      "epoch: 145 loss:  0.051\n",
      "epoch: 146 loss:  0.050\n",
      "epoch: 147 loss:  0.050\n",
      "epoch: 148 loss:  0.049\n",
      "epoch: 149 loss:  0.049\n",
      "epoch: 150 loss:  0.049\n",
      "epoch: 151 loss:  0.048\n",
      "epoch: 152 loss:  0.048\n",
      "epoch: 153 loss:  0.047\n",
      "epoch: 154 loss:  0.047\n",
      "epoch: 155 loss:  0.047\n",
      "epoch: 156 loss:  0.046\n",
      "epoch: 157 loss:  0.046\n",
      "epoch: 158 loss:  0.046\n",
      "epoch: 159 loss:  0.045\n",
      "epoch: 160 loss:  0.045\n",
      "epoch: 161 loss:  0.044\n",
      "epoch: 162 loss:  0.044\n",
      "epoch: 163 loss:  0.044\n",
      "epoch: 164 loss:  0.043\n",
      "epoch: 165 loss:  0.043\n",
      "epoch: 166 loss:  0.043\n",
      "epoch: 167 loss:  0.042\n",
      "epoch: 168 loss:  0.042\n",
      "epoch: 169 loss:  0.042\n",
      "epoch: 170 loss:  0.041\n",
      "epoch: 171 loss:  0.041\n",
      "epoch: 172 loss:  0.041\n",
      "epoch: 173 loss:  0.040\n",
      "epoch: 174 loss:  0.040\n",
      "epoch: 175 loss:  0.040\n",
      "epoch: 176 loss:  0.039\n",
      "epoch: 177 loss:  0.039\n",
      "epoch: 178 loss:  0.039\n",
      "epoch: 179 loss:  0.039\n",
      "epoch: 180 loss:  0.038\n",
      "epoch: 181 loss:  0.038\n",
      "epoch: 182 loss:  0.038\n",
      "epoch: 183 loss:  0.037\n",
      "epoch: 184 loss:  0.037\n",
      "epoch: 185 loss:  0.037\n",
      "epoch: 186 loss:  0.037\n",
      "epoch: 187 loss:  0.036\n",
      "epoch: 188 loss:  0.036\n",
      "epoch: 189 loss:  0.036\n",
      "epoch: 190 loss:  0.035\n",
      "epoch: 191 loss:  0.035\n",
      "epoch: 192 loss:  0.035\n",
      "epoch: 193 loss:  0.035\n",
      "epoch: 194 loss:  0.034\n",
      "epoch: 195 loss:  0.034\n",
      "epoch: 196 loss:  0.034\n",
      "epoch: 197 loss:  0.034\n",
      "epoch: 198 loss:  0.033\n",
      "epoch: 199 loss:  0.033\n",
      "epoch: 200 loss:  0.033\n"
     ]
    }
   ],
   "source": [
    "nepochs = 200    # number of epochs\n",
    "results_path = 'linear2layer200epochs.pt'\n",
    "\n",
    "# initialise ndarray to store the mean loss in each epoch (on the training data)\n",
    "losses = np.zeros(nepochs)\n",
    "\n",
    "# Use a loss function and optimiser provided as part of PyTorch.\n",
    "# The chosen optimiser (Stochastic Gradient Descent with momentum) needs only to be given the parameters (weights and biases)\n",
    "# of the network and updates these when asked to perform an optimisation step below.\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # Note, this takes log(Softmax) and Log-Likelihood\n",
    "                                # Meaning, it takes out the prob. of an input to be equal to a certain output\n",
    "                                # and tell us of the loss..how much it is not so\n",
    "optimizer = optim.SGD(net_2layer.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "\n",
    "    # initialise variables for mean loss calculation\n",
    "    running_loss = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # Zero the parameter gradients to remove accumulated gradient from a previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward, backward, and update parameters\n",
    "        outputs = net_2layer(inputs) # A map of input(784) - output(10) is made..\n",
    "                              # connections have what is called weights/coeffs, imagine like a computational graph\n",
    "                              # each of 784 points connected to 10 points, we need gradient of each now,\n",
    "                              # To Optimise.\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward() # Assigns gradients to each connection/weight/coeff. of the input - output in a Matrix\n",
    "        optimizer.step() # Updates the matrix with new weights, considering the step size..like p = p - stepsize*grad\n",
    "\n",
    "        # accumulate loss and increment minibatches\n",
    "        running_loss += loss.item()\n",
    "        n += 1\n",
    "       \n",
    "    # record the mean loss for this epoch and show progress\n",
    "    losses[epoch] = running_loss / n\n",
    "    print(f\"epoch: {epoch+1} loss: {losses[epoch] : .3f}\")\n",
    "    \n",
    "# save network parameters and losses\n",
    "torch.save({\"state_dict\": net_2layer.state_dict(), \"losses\": losses}, results_path)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the history of the loss function during training (mean loss in each epoch) for 1 and 2 layer models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d1 = torch.load('linear1layer200epochs.pt')\n",
    "d2 = torch.load('linear2layer200epochs.pt')\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(d1[\"losses\"], 'r', label = '1 layer', )\n",
    "plt.plot(d2[\"losses\"], 'g', label = '2 layers' )\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training loss for 1 and 2 layer classifiers')\n",
    "\n",
    "fig.savefig(\"training_loss_MNIST.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the trained classifier `net` perform on the test set? First define our performance measures in terms of a given confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0147, -0.0060,  0.0245,  ...,  0.0179, -0.0320, -0.0295],\n",
      "        [-0.0138,  0.0224,  0.0335,  ..., -0.0325, -0.0247, -0.0065],\n",
      "        [ 0.0145,  0.0156,  0.0193,  ..., -0.0311, -0.0173, -0.0155],\n",
      "        ...,\n",
      "        [-0.0191, -0.0318,  0.0323,  ..., -0.0335, -0.0066,  0.0075],\n",
      "        [-0.0214,  0.0071, -0.0070,  ..., -0.0027, -0.0098,  0.0214],\n",
      "        [ 0.0061,  0.0247, -0.0222,  ...,  0.0278,  0.0174,  0.0195]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.0097e-01, -2.5282e-02,  5.4313e-02,  3.3446e-01, -8.2347e-02,\n",
      "         4.2241e-02, -2.2882e-01,  8.2888e-02,  7.6454e-02, -6.0717e-02,\n",
      "        -1.0105e-01,  1.1075e-01,  3.0702e-02,  9.0859e-02, -6.1725e-02,\n",
      "         4.8058e-03, -8.5959e-02, -1.6576e-02,  5.4720e-03,  7.8994e-02,\n",
      "        -3.2773e-02, -1.5465e-02,  4.7161e-02, -2.6032e-02, -2.5957e-02,\n",
      "        -3.3975e-02, -6.7332e-02,  2.0216e-01, -2.1667e-02,  7.9158e-02,\n",
      "        -3.9701e-02, -2.4107e-01, -1.9887e-01, -1.2936e-01,  2.6201e-02,\n",
      "         1.8495e-01,  8.9024e-03,  2.5345e-02, -9.0492e-02,  1.0071e-01,\n",
      "         1.0985e-01, -5.3323e-02, -1.1136e-01, -2.6705e-01,  3.3583e-02,\n",
      "        -3.2785e-02,  2.3644e-02, -6.1324e-02,  1.7505e-01, -4.1800e-02,\n",
      "         3.1844e-01, -9.8242e-02, -9.2647e-02, -1.6606e-02, -1.8259e-02,\n",
      "         1.2981e-01, -2.4495e-01,  1.7340e-01,  2.8135e-02,  2.7111e-01,\n",
      "        -1.1759e-01,  2.1481e-01, -2.0610e-01, -1.5023e-01,  4.3222e-02,\n",
      "        -1.6466e-01, -2.1471e-02,  2.2993e-01, -5.4856e-02, -1.1658e-01,\n",
      "         1.6747e-01, -7.8666e-02, -3.8794e-02, -3.4348e-02,  5.1173e-02,\n",
      "        -4.4835e-02, -4.3695e-02, -5.7695e-02,  1.9704e-02, -1.2074e-01,\n",
      "        -1.2378e-04, -5.1026e-02, -4.8538e-02,  2.1118e-01,  2.7809e-01,\n",
      "        -3.1286e-01, -3.8489e-01,  3.6305e-02,  1.7376e-01, -1.8406e-01,\n",
      "        -1.7952e-02, -5.5090e-02,  1.0081e-02, -1.2630e-01, -4.5530e-02,\n",
      "         9.1289e-02, -1.5852e-01,  1.6618e-01,  1.5384e-01,  4.9647e-02,\n",
      "        -1.2812e-01,  5.2876e-02, -2.0468e-02,  1.5303e-01, -9.3398e-02,\n",
      "        -4.2572e-01, -4.8914e-02,  3.6840e-02,  2.5467e-01,  4.4072e-02,\n",
      "        -1.7449e-02,  1.2495e-01, -4.2184e-02,  1.5598e-02, -1.3237e-01,\n",
      "        -8.8975e-02, -2.4568e-02,  2.0219e-01,  3.2082e-02, -8.0474e-02,\n",
      "         5.0567e-02,  1.1769e-02, -4.8935e-02,  5.4999e-02, -2.2583e-02,\n",
      "        -1.6690e-01, -2.6968e-02, -2.9938e-01, -6.5408e-02,  1.9460e-02,\n",
      "        -2.1800e-02,  1.5354e-01, -1.8409e-02,  2.3145e-02, -1.7261e-02,\n",
      "         7.3168e-03, -1.9166e-01,  1.5431e-01,  1.0437e-01, -1.5483e-02,\n",
      "        -1.7469e-01, -2.8134e-01,  1.6206e-01, -1.6561e-01,  1.5817e-01,\n",
      "         5.5174e-02, -2.2585e-02, -1.0388e-01, -2.7925e-01, -2.1382e-02,\n",
      "         9.0580e-02, -4.2892e-02, -3.0667e-01,  3.3055e-01, -3.1271e-01,\n",
      "         1.7006e-01,  2.4887e-02, -9.0807e-02, -1.2974e-01, -2.1728e-01,\n",
      "        -7.5671e-02,  5.3191e-02,  1.3926e-01, -1.9170e-02, -1.0044e-01,\n",
      "         1.1596e-02, -2.2697e-01, -6.3312e-02, -1.1887e-01, -1.9979e-01,\n",
      "         9.7097e-02,  6.7890e-02, -4.9210e-02, -1.7736e-01,  2.0408e-01,\n",
      "        -9.3755e-02, -5.4028e-02,  5.3521e-02, -1.1340e-01,  1.1924e-01,\n",
      "         1.2888e-01, -5.9975e-02, -6.0380e-02, -1.3204e-01, -3.4834e-01,\n",
      "        -6.7676e-02,  3.8357e-02,  3.9137e-03,  3.6033e-02, -1.1083e-01,\n",
      "         2.3455e-02, -2.0558e-01,  2.1027e-01,  1.3216e-01,  9.2800e-02,\n",
      "        -6.0579e-02,  1.5413e-01, -1.6253e-01, -1.1539e-01, -2.4200e-02,\n",
      "        -5.4401e-02, -2.0049e-02, -1.3533e-02, -2.4146e-02,  4.5460e-01,\n",
      "         1.2451e-01, -2.8182e-03,  1.3098e-01, -9.0311e-03, -6.7447e-02,\n",
      "         1.3205e-04, -4.8424e-02, -1.2080e-01, -1.6839e-01,  3.1634e-03,\n",
      "        -8.3049e-02, -1.6644e-03,  8.1489e-02,  1.5382e-02, -5.5036e-03,\n",
      "        -1.3622e-01, -2.7748e-02, -2.6336e-02,  5.2418e-02, -2.9662e-02,\n",
      "         1.7684e-01,  7.6729e-02,  1.4016e-02,  1.3563e-01, -4.1316e-02,\n",
      "         1.7561e-02,  1.5281e-01,  3.0186e-02,  1.3484e-01, -2.3576e-02,\n",
      "         1.5416e-01, -3.2569e-02,  2.5652e-01, -3.2292e-02, -7.4368e-02,\n",
      "        -6.7670e-02,  2.6738e-02, -3.9545e-02, -7.2076e-02, -1.0347e-01,\n",
      "        -7.3030e-02,  1.4754e-01, -4.2764e-02, -7.9917e-02,  5.7389e-03,\n",
      "        -6.7414e-03, -1.0235e-01,  8.1612e-02,  3.8365e-02, -1.8123e-01,\n",
      "        -2.1985e-03, -3.2069e-02,  7.5020e-02, -1.3019e-01,  2.5572e-02,\n",
      "         1.2209e-02,  2.5912e-02,  1.6565e-01, -3.0917e-02,  5.4014e-02,\n",
      "         5.6556e-03,  1.3988e-01, -2.9261e-01,  1.2902e-01, -1.0019e-01,\n",
      "        -4.6152e-02,  3.6686e-01, -4.6284e-03, -1.4138e-01, -2.4668e-04,\n",
      "         1.1939e-01,  1.6467e-02,  1.3357e-01,  3.1901e-02,  9.2639e-02,\n",
      "        -4.1054e-02,  1.2192e-01, -6.0295e-02, -9.5092e-03, -3.2165e-02,\n",
      "         3.4777e-02, -2.9925e-02,  1.4925e-01, -1.4871e-01, -1.3079e-01,\n",
      "        -7.2960e-02,  1.7031e-01, -1.5691e-02,  2.3042e-02,  2.9781e-01,\n",
      "        -3.9280e-01,  1.2334e-01, -5.4257e-02, -3.7314e-02, -1.4698e-01],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.6051, -0.1050, -0.1939,  ...,  0.1632, -0.5967,  0.3696],\n",
      "        [ 0.7440, -0.0099,  0.0325,  ...,  0.3441,  0.1236,  0.0673],\n",
      "        [ 0.0241,  0.2580, -0.4807,  ...,  0.2721,  0.3909, -0.6254],\n",
      "        ...,\n",
      "        [-0.2875, -0.4748, -0.0378,  ...,  0.6809, -0.0348,  0.5003],\n",
      "        [-0.2135,  0.3494,  0.4697,  ...,  0.4162, -0.3489,  0.2013],\n",
      "        [-0.4255,  0.1718, -0.7242,  ..., -0.2411,  0.1763, -0.6622]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0082,  0.0489,  0.0187, -0.0207,  0.0162,  0.0352, -0.0572,  0.0084,\n",
      "         0.0240,  0.0376], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Weights, w from wx+b\n",
    "# Bias, b from wx+b\n",
    "for  i in net_2layer.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(cnfm):\n",
    "    return cnfm.trace()/cnfm.sum((0,1))\n",
    "\n",
    "def recalls(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(1)\n",
    "\n",
    "def precisions(cnfm):\n",
    "    return np.diag(cnfm)/cnfm.sum(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on test data, build a confusion matrix and compute performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[ 960    0    1    3    0    7    4    4    1    0]\n",
      " [   0 1114    3    1    0    2    4    2    9    0]\n",
      " [   6   10  929   16    9    4   13    9   31    5]\n",
      " [   4    1   15  924    0   27    3   11   17    8]\n",
      " [   1    2    6    3  918    0    8    4    9   31]\n",
      " [  10    2    3   33    9  788   13    5   26    3]\n",
      " [   9    3    8    2    8   18  907    2    1    0]\n",
      " [   1    8   22    8    8    1    0  950    2   28]\n",
      " [   9   12    8   25    9   34   11    8  845   13]\n",
      " [  11    8    1   10   26    7    0   20    6  920]]\n",
      "Accuracy: 92.5%\n",
      "Class 0 : Precision 95.0%  Recall 98.0%\n",
      "Class 1 : Precision 96.0%  Recall 98.1%\n",
      "Class 2 : Precision 93.3%  Recall 90.0%\n",
      "Class 3 : Precision 90.1%  Recall 91.5%\n",
      "Class 4 : Precision 93.0%  Recall 93.5%\n",
      "Class 5 : Precision 88.7%  Recall 88.3%\n",
      "Class 6 : Precision 94.2%  Recall 94.7%\n",
      "Class 7 : Precision 93.6%  Recall 92.4%\n",
      "Class 8 : Precision 89.2%  Recall 86.8%\n",
      "Class 9 : Precision 91.3%  Recall 91.2%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d = torch.load('linear1layer200epochs.pt')\n",
    "# now load the parameter state into the current model (make sure this is the right model).\n",
    "net_1layer.load_state_dict(d[\"state_dict\"])\n",
    "\n",
    "# initialise confusion matrix\n",
    "nclasses = classes.shape[0]\n",
    "cnfm = np.zeros((nclasses,nclasses),dtype=int)\n",
    "\n",
    "# work without gradient computation since we are testing (i.e. no optimisation)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net_1layer(images)\n",
    "        \n",
    "        # find the class with the highest output.\n",
    "        # note that the outputs are confidence values since we didn't need to apply softmax in our network\n",
    "        # (nn.crossentropyloss takes raw condifence values and does its own softmax)   \n",
    "        _, predicted = torch.max(outputs, 1)    \n",
    "       \n",
    "        \n",
    "        # accumulate into confusion matrix\n",
    "        for i in range(labels.size(0)):\n",
    "            cnfm[labels[i].item(),predicted[i].item()] += 1\n",
    "              \n",
    "print(\"Confusion matrix\")\n",
    "print(cnfm)\n",
    "\n",
    "# show confusion matrix as a grey-level image\n",
    "plt.imshow(cnfm, cmap='gray')\n",
    "\n",
    "# show per-class recall and precision\n",
    "print(f\"Accuracy: {accuracy(cnfm) :.1%}\")\n",
    "r = recalls(cnfm)\n",
    "p = precisions(cnfm)\n",
    "for i in range(nclasses):\n",
    "    print(f\"Class {classes[i]} : Precision {p[i] :.1%}  Recall {r[i] :.1%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
